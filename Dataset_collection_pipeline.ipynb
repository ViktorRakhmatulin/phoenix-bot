{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2884d030",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/viktor/Annotator/Dataset_collection_pipeline.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/viktor/Annotator/Dataset_collection_pipeline.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m keypoints, anchors \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_points_from_CAD()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/viktor/Annotator/Dataset_collection_pipeline.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m normalized_keypoints \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mnormalize_and_orient_kpts(keypoints)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/viktor/Annotator/Dataset_collection_pipeline.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m normalized_anchors \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mnormalize_and_orient_kpts(anchors)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/viktor/Annotator/Dataset_collection_pipeline.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m anchor_ground_truth \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/viktor/Annotator/Dataset_collection_pipeline.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ([[\u001b[39m606\u001b[39m, \u001b[39m323\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/viktor/Annotator/Dataset_collection_pipeline.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m        [\u001b[39m571\u001b[39m, \u001b[39m277\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/viktor/Annotator/Dataset_collection_pipeline.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m        [\u001b[39m571\u001b[39m, \u001b[39m383\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/viktor/Annotator/Dataset_collection_pipeline.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m        [\u001b[39m640\u001b[39m, \u001b[39m383\u001b[39m]]))\n",
      "File \u001b[0;32m~/Annotator/annotation_utils.py:117\u001b[0m, in \u001b[0;36mnormalize_and_orient_kpts\u001b[0;34m(points_in_CAD)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m points_in_CAD\n\u001b[1;32m    115\u001b[0m points_normalized \u001b[39m=\u001b[39m points_in_CAD\u001b[39m/\u001b[39m\u001b[39m48\u001b[39m\n\u001b[0;32m--> 117\u001b[0m points_normalized \u001b[39m=\u001b[39m correct_CAD_kpts_orientation(points_normalized)\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m points_normalized\n",
      "File \u001b[0;32m~/Annotator/annotation_utils.py:112\u001b[0m, in \u001b[0;36mnormalize_and_orient_kpts.<locals>.correct_CAD_kpts_orientation\u001b[0;34m(points_in_CAD)\u001b[0m\n\u001b[1;32m    110\u001b[0m rot \u001b[39m=\u001b[39m rot_mtx(np\u001b[39m.\u001b[39mdeg2rad(\u001b[39m180\u001b[39m))\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m i,kp \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(points_in_CAD):\n\u001b[0;32m--> 112\u001b[0m     points_in_CAD[i] \u001b[39m=\u001b[39m rot \u001b[39m@\u001b[39;49m kp\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m points_in_CAD\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)"
     ]
    }
   ],
   "source": [
    "import annotation_utils as utils\n",
    "import numpy as np\n",
    "import cv2\n",
    "np.set_printoptions(suppress=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "camera_params = [633.0902963939839, 638.754439101168,\n",
    "                 629.0646296262861, 362.7401326185789]\n",
    "\n",
    "keypoints, anchors = utils.get_points_from_CAD()\n",
    "\n",
    "normalized_keypoints = utils.normalize_and_orient_kpts(keypoints)\n",
    "normalized_anchors = utils.normalize_and_orient_kpts(anchors)\n",
    "\n",
    "anchor_ground_truth = np.array(\n",
    "([[606, 323],\n",
    "       [571, 277],\n",
    "       [642, 277],\n",
    "       [536, 323],\n",
    "       [676, 322],\n",
    "       [571, 383],\n",
    "       [640, 383]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d90c3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import ProjectiveTransform, warp\n",
    "from skimage import transform\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def get_corrected_normalized_keypoints(res_warp,\n",
    "                                        anchors_points_normalized,\n",
    "                                        anchor_ground_truth,\n",
    "                                        normalized_keypoints):\n",
    "    \n",
    "\n",
    "    def get_corrective_tf(anchor_marker, anchor_gt):\n",
    "        corrective_tf = cv2.estimateAffine2D(anchor_marker, anchor_gt,ransacReprojThreshold = 2)\n",
    "        return corrective_tf\n",
    "    \n",
    "    def debug_pts(keypoints, comment):\n",
    "        no_marker_name = 'no_marker_close_new_2.jpg'\n",
    "        img_nm = cv2.imread(no_marker_name)\n",
    "\n",
    "        for kp in keypoints:\n",
    "            cv2.circle(img_nm, (int(kp[0]), int(kp[1])), 2, (0,0,255), -1)\n",
    "        cv2.imshow(comment, img_nm)\n",
    "        cv2.waitKey(0) \n",
    "    \n",
    "    anchors_points = get_keypoints_on_warped_image(res_warp, anchors_points_normalized)\n",
    "    keypoints = get_keypoints_on_warped_image(res_warp, normalized_keypoints)\n",
    "    # print(keypoints)\n",
    "    corrective_tf, inliers = get_corrective_tf(anchors_points,anchor_ground_truth)\n",
    "    \n",
    "#     tform = ProjectiveTransform()\n",
    "    tform = transform.estimate_transform('euclidean', anchors_points,anchor_ground_truth)\n",
    "#     tform.estimate('euclidean',anchors_points,anchor_ground_truth)\n",
    "    print(\"tform.params\")\n",
    "    print(tform.params)\n",
    "    print(\"resulting transform to correct points\")\n",
    "    print(corrective_tf)\n",
    "    print(\"inliers\", inliers)\n",
    "    dx = (res_warp[0].corners[2] - res_warp[0].corners[1])\n",
    "    center = (res_warp[0].corners[1] + res_warp[0].corners[3])/2\n",
    "    scale = dx[0]\n",
    "    homog_keypoints = np.hstack((keypoints,np.ones((keypoints.shape[0],1))))\n",
    "    # print(tform.params @ homog_keypoints.T)\n",
    "    corrective_tf = tform.params[:2]\n",
    "    corrected_points =(corrective_tf @ homog_keypoints.T).T\n",
    "    # print(corrected_points)\n",
    "    # print((corrective_tf @ homog_keypoints.T).T)\n",
    "    corrective_tf = tform.params[:2]\n",
    "    transformed_anchors_points = (corrective_tf @  np.hstack((anchors_points,np.ones((anchors_points.shape[0],1)))).T).T\n",
    "\n",
    "    debug_pts(anchor_ground_truth,\"anchor_ground_truth\")\n",
    "\n",
    "#     for p in corrected_points:\n",
    "# #         print(p)\n",
    "#         p/=p[2]\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    corrected_normalized_kpts =  (corrected_points - center)/ scale\n",
    "\n",
    "    return corrected_points, corrected_normalized_kpts,transformed_anchors_points\n",
    "\n",
    "\n",
    "def get_keypoints_on_warped_image(res_warp, normalized_kpts):\n",
    "    dx = (res_warp[0].corners[2] - res_warp[0].corners[1])\n",
    "    center = (res_warp[0].corners[1] + res_warp[0].corners[3])/2\n",
    "    scale = dx[0] \n",
    "    keypoints = normalized_kpts * scale + center\n",
    "    \n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def get_keypoints_on_image(img, corrected_normalized_keypoints,camera_params):\n",
    "    \"\"\"\n",
    "    detects keypoints on image, with image warp and all the stuff, \n",
    "    returns points' coords on original image \n",
    "    \"\"\"\n",
    "    res = utils.detect_apriltag(img,camera_params)\n",
    "    \n",
    "    img_warped, desired_corners,tform = warp_img(img, res)\n",
    "    res_warp = utils.detect_apriltag(img_warped,camera_params)\n",
    "    warped_keypoints = utils.get_keypoints_on_warped_image(res_warp, corrected_normalized_keypoints)\n",
    "    desired_corners = utils.get_straight_corners(res_warp)\n",
    "#     print(\"desired_corners\", desired_corners)\n",
    "    \n",
    "    # tform = ProjectiveTransform()\n",
    "    # estimate(src, dst)\n",
    "    # tform.estimate(c_to, c_from)\n",
    "\n",
    "    str_project = get_projection(desired_corners, res[0].corners)\n",
    "    print(str_project.params)\n",
    "    points_on_image = [str_project.params @ np.array([p[0], p[1], 1]) for p in warped_keypoints]\n",
    "    for p in points_on_image:\n",
    "        p/= p[2]\n",
    "    \n",
    "    return np.array(points_on_image)\n",
    " \n",
    "def warp_img(image, res):\n",
    "    \"\"\"warps image so marker would be square\"\"\"\n",
    "    desired_corners = utils.get_straight_corners(res)\n",
    "    print(desired_corners, res[0].corners)\n",
    "    tform = ProjectiveTransform()\n",
    "    inverse_tf = tform.estimate(desired_corners, res[0].corners)\n",
    "\n",
    "    image_warped = warp(image, inverse_tf)\n",
    "    return image_warped, desired_corners, tform\n",
    "\n",
    "\n",
    "def draw_corners(img, r, show=False):\n",
    "    \"\"\"draws marker corners, returns them\"\"\"\n",
    "    if isinstance(r, np.ndarray):\n",
    "        imagePoints = [r]\n",
    "    else:\n",
    "        imagePoints = r.corners.reshape(1,4,2)\n",
    "    \n",
    "    for corner in range(np.size(imagePoints[0],axis=0)):\n",
    "        center = ((int(imagePoints[0][corner][0]),int(imagePoints[0][corner][1])))\n",
    "        cv2.circle(img, center, 3, (0,0,255), -1)\n",
    "    \n",
    "    if show:\n",
    "        cv2.imshow('img', color_image)\n",
    "    return imagePoints[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b093f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Detection object:\n",
      "tag_family = b'tagStandard52h13'\n",
      "tag_id = 48701\n",
      "hamming = 0\n",
      "decision_margin = 58.5472297668457\n",
      "homography = [[   0.37483707 -107.18212331  602.57440465]\n",
      " [ 105.41946277    1.02560177  330.03666681]\n",
      " [  -0.00147215   -0.00122736    1.        ]]\n",
      "center = [602.57440465 330.03666681]\n",
      "corners = [[494.89630127 225.58758545]\n",
      " [497.10906982 437.66320801]\n",
      " [710.30523682 434.53689575]\n",
      " [707.47186279 222.98963928]]\n",
      "pose_R = [[ 0.01351805 -0.99984746 -0.01105942]\n",
      " [ 0.99968805  0.01328193  0.02115189]\n",
      " [-0.02100177 -0.01134191  0.9997151 ]]\n",
      "pose_t = [[-0.0059953 ]\n",
      " [-0.00740351]\n",
      " [ 0.14366104]]\n",
      "pose_err = 1.8576139409624874e-07\n",
      "]\n",
      "[[496 224]\n",
      " [496 436]\n",
      " [708 224]\n",
      " [708 436]] [[494.89630127 225.58758545]\n",
      " [497.10906982 437.66320801]\n",
      " [707.47186279 222.98963928]\n",
      " [710.30523682 434.53689575]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m res \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdetect_apriltag(img,camera_params)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X52sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(res)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X52sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m img_warped, desired_corners,tform \u001b[39m=\u001b[39m warp_img(img,res)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X52sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m utils\u001b[39m.\u001b[39mshow_image_with_points(img_warped,anchor_ground_truth, \u001b[39m\"\u001b[39m\u001b[39mimg_warped\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X52sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;32m/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb Cell 4\u001b[0m in \u001b[0;36mwarp_img\u001b[0;34m(image, res)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X52sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m tform \u001b[39m=\u001b[39m ProjectiveTransform()\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X52sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m inverse_tf \u001b[39m=\u001b[39m tform\u001b[39m.\u001b[39mestimate(desired_corners, res[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcorners)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X52sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m image_warped \u001b[39m=\u001b[39m warp(image, inverse_tf)\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X52sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mreturn\u001b[39;00m image_warped, desired_corners, tform\n",
      "File \u001b[0;32m~/anaconda/envs/CameraCalibrationOptimize/lib/python3.9/site-packages/skimage/transform/_warps.py:988\u001b[0m, in \u001b[0;36mwarp\u001b[0;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    981\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(input_shape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(output_shape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    982\u001b[0m         \u001b[39m# Input image is 2D and has color channel, but output_shape is\u001b[39;00m\n\u001b[1;32m    983\u001b[0m         \u001b[39m# given for 2-D images. Automatically add the color channel\u001b[39;00m\n\u001b[1;32m    984\u001b[0m         \u001b[39m# dimensionality.\u001b[39;00m\n\u001b[1;32m    985\u001b[0m         output_shape \u001b[39m=\u001b[39m (output_shape[\u001b[39m0\u001b[39m], output_shape[\u001b[39m1\u001b[39m],\n\u001b[1;32m    986\u001b[0m                         input_shape[\u001b[39m2\u001b[39m])\n\u001b[0;32m--> 988\u001b[0m     coords \u001b[39m=\u001b[39m warp_coords(coord_map, output_shape)\n\u001b[1;32m    990\u001b[0m \u001b[39m# Pre-filtering not necessary for order 0, 1 interpolation\u001b[39;00m\n\u001b[1;32m    991\u001b[0m prefilter \u001b[39m=\u001b[39m order \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda/envs/CameraCalibrationOptimize/lib/python3.9/site-packages/skimage/transform/_warps.py:675\u001b[0m, in \u001b[0;36mwarp_coords\u001b[0;34m(coord_map, shape, dtype)\u001b[0m\n\u001b[1;32m    671\u001b[0m tf_coords \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mindices((cols, rows), dtype\u001b[39m=\u001b[39mdtype)\u001b[39m.\u001b[39mreshape(\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mT\n\u001b[1;32m    673\u001b[0m \u001b[39m# Map each (row, col) pair to the source image according to\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39m# the user-provided mapping\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m tf_coords \u001b[39m=\u001b[39m coord_map(tf_coords)\n\u001b[1;32m    677\u001b[0m \u001b[39m# Reshape back to a (2, M, N) coordinate grid\u001b[39;00m\n\u001b[1;32m    678\u001b[0m tf_coords \u001b[39m=\u001b[39m tf_coords\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cols, rows))\u001b[39m.\u001b[39mswapaxes(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda/envs/CameraCalibrationOptimize/lib/python3.9/site-packages/skimage/transform/_warps.py:979\u001b[0m, in \u001b[0;36mwarp.<locals>.coord_map\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcoord_map\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 979\u001b[0m     \u001b[39mreturn\u001b[39;00m inverse_map(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmap_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "# img = cv2.imread(img_with_marker_name)\n",
    "img_with_marker_name = 'marker_close_new_2.jpg'\n",
    "\n",
    "# img = cv2.imread(img_with_marker_name)\n",
    "\n",
    "# utils.show_image_with_points(img,anchor_ground_truth, \"kpts_on_marker\")\n",
    "img = cv2.imread(img_with_marker_name)\n",
    "res = utils.detect_apriltag(img,camera_params)\n",
    "print(res)\n",
    "img_warped, desired_corners,tform = warp_img(img,res)\n",
    "\n",
    "utils.show_image_with_points(img_warped,anchor_ground_truth, \"img_warped\")\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3d00bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f8a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def warp_img(image, res):\n",
    "#     \"\"\"warps image so marker would be square\"\"\"\n",
    "#     desired_corners = get_straight_corners(res)\n",
    "\n",
    "#     tform = get_projection(desired_corners, res[0].corners)\n",
    "#     image_warped = warp(image, tform)\n",
    "#     return image_warped, desired_corners, tform\n",
    "\n",
    "utils.show_image_with_points(img,points_on_image, \"kpts_on_marker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a37af",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f7c845",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor difference in pix [[-4.62428663  7.01455117]\n",
      " [-4.26832825  3.02692779]\n",
      " [-4.85491859  3.94094286]\n",
      " [-4.93881626  6.07484948]\n",
      " [-4.18292562  8.95076093]\n",
      " [-5.48483767  7.73925008]\n",
      " [-4.16732841  8.71351528]]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.4) ../modules/calib3d/src/ptsetreg.cpp:1022: error: (-215:Assertion failed) count >= 0 && to.checkVector(2) == count in function 'estimateAffine2D'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39manchor difference in pix\u001b[39m\u001b[39m\"\u001b[39m, anchor_on_image\u001b[39m-\u001b[39mpoints_on_image)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# calculates corrected points from 1 shot\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m kpts_transformed,corrected_normalized_keypoints,transformed_anchors_points \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mget_corrected_normalized_keypoints(res_warp,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                                         normalized_anchors,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                                         points_on_image,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                                         normalized_keypoints)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/viktor/E/Annotator/Dataset_collection_pipeline.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n",
      "File \u001b[0;32m/media/viktor/E/Annotator/annotation_utils.py:203\u001b[0m, in \u001b[0;36mget_corrected_normalized_keypoints\u001b[0;34m(res_warp, anchors_points_normalized, anchor_ground_truth, normalized_keypoints)\u001b[0m\n\u001b[1;32m    201\u001b[0m     keypoints \u001b[39m=\u001b[39m get_keypoints_on_warped_image(res_warp, normalized_keypoints)\n\u001b[1;32m    202\u001b[0m     \u001b[39m# print(keypoints)\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     corrective_tf, inliers \u001b[39m=\u001b[39m get_corrective_tf(anchors_points,anchor_ground_truth)\n\u001b[1;32m    205\u001b[0m \u001b[39m#     tform = ProjectiveTransform()\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     tform \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39mestimate_transform(\u001b[39m'\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m'\u001b[39m, anchors_points,anchor_ground_truth)\n",
      "File \u001b[0;32m/media/viktor/E/Annotator/annotation_utils.py:197\u001b[0m, in \u001b[0;36mget_corrected_normalized_keypoints.<locals>.get_corrective_tf\u001b[0;34m(anchor_marker, anchor_gt)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_corrective_tf\u001b[39m(anchor_marker, anchor_gt):\n\u001b[0;32m--> 197\u001b[0m     corrective_tf \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mestimateAffine2D(anchor_marker, anchor_gt,ransacReprojThreshold \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m)\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m corrective_tf\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) ../modules/calib3d/src/ptsetreg.cpp:1022: error: (-215:Assertion failed) count >= 0 && to.checkVector(2) == count in function 'estimateAffine2D'\n"
     ]
    }
   ],
   "source": [
    "cv2.destroyAllWindows()\n",
    "\n",
    "# we take photos from as close as possible\n",
    "img_with_marker_name = 'marker_close_new_2.jpg'\n",
    "\n",
    "\n",
    "img = cv2.imread(img_with_marker_name)\n",
    "\n",
    "res = utils.detect_apriltag(img, camera_params)\n",
    "img_warped, desired_corners,tform = utils.warp_img(img, res)\n",
    "\n",
    "# print(str_project.params)\n",
    "points_on_image = [tform.params @ np.array([p[0], p[1], 1]) for p in anchor_ground_truth]\n",
    "for p in points_on_image:\n",
    "    p/= p[2]\n",
    "points_on_image = np.array(points_on_image)\n",
    "res_warp = utils.detect_apriltag(img_warped,camera_params)\n",
    "\n",
    "anchor_on_image = utils.get_keypoints_on_warped_image(res_warp, normalized_anchors)\n",
    "print(\"anchor difference in pix\", anchor_on_image-anchor_ground_truth)\n",
    "\n",
    "# calculates corrected points from 1 shot\n",
    "kpts_transformed,corrected_normalized_keypoints,transformed_anchors_points = utils.get_corrected_normalized_keypoints(res_warp,\n",
    "                                        normalized_anchors,\n",
    "                                        anchor_ground_truth,\n",
    "                                        normalized_keypoints)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "# corrected_normalised_keypoints is what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef535e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50720594, -0.01005746],\n",
       "       [-0.1740037 ,  0.52221415],\n",
       "       [ 0.06308917, -0.39180091],\n",
       "       [ 0.17123305,  0.92964424],\n",
       "       [ 0.71634745, -0.94626722],\n",
       "       [ 1.04250572,  0.46624145],\n",
       "       [ 1.37549899, -0.50802375]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_on_image[:,:2] - anchor_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79dad784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e45073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.01357425  0.00354467 -6.30649152]\n",
      " [-0.00969867  0.99690554  8.6742203 ]\n",
      " [ 0.00001161 -0.00001392  1.        ]]\n",
      "[[-0.41704922  1.79105774]\n",
      " [-0.31745568  1.16642855]\n",
      " [-0.35849922  2.73184072]\n",
      " [ 0.20766572 -0.13745943]\n",
      " [-0.00242317  1.96303519]\n",
      " [ 0.28353443  0.56524883]\n",
      " [ 0.03761471  2.66235291]\n",
      " [ 0.74909092 -0.36998423]\n",
      " [ 0.3054197   3.21134274]\n",
      " [ 1.56076472 -1.19939394]\n",
      " [ 2.32982278  2.33707719]\n",
      " [ 3.32199326 -0.23767849]\n",
      " [ 2.40864421  2.40025547]\n",
      " [ 3.44826556 -0.25780689]]\n"
     ]
    }
   ],
   "source": [
    "# kpts_transformed - normalized_keypoints\n",
    "\n",
    "keypoints = utils.get_keypoints_on_image(img_m, corrected_normalized_keypoints, camera_params)\n",
    "print(keypoints[:,:2] - kpts_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b54b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41704922  1.79105774]\n",
      " [-0.31745568  1.16642855]\n",
      " [-0.35849922  2.73184072]\n",
      " [ 0.20766572 -0.13745943]\n",
      " [-0.00242317  1.96303519]\n",
      " [ 0.28353443  0.56524883]\n",
      " [ 0.03761471  2.66235291]\n",
      " [ 0.74909092 -0.36998423]\n",
      " [ 0.3054197   3.21134274]\n",
      " [ 1.56076472 -1.19939394]\n",
      " [ 2.32982278  2.33707719]\n",
      " [ 3.32199326 -0.23767849]\n",
      " [ 2.40864421  2.40025547]\n",
      " [ 3.44826556 -0.25780689]]\n"
     ]
    }
   ],
   "source": [
    "print(keypoints[:,:2] - kpts_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e6dba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marker_name = f'/home/viacheslav/jupyter_notebooks/data/far_photos_with_marker/{img_num}.jpg'\n",
    "# no_marker_name = f'/home/viacheslav/jupyter_notebooks/data/far_photos/{img_num}.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_w, warp_res, _ = utils.warp_img(img_m, utils.detect_apriltag(img_m, camera_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a23e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cv2.imshow('w', img_w)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a0d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.01357425  0.00354467 -6.30649152]\n",
      " [-0.00969867  0.99690554  8.6742203 ]\n",
      " [ 0.00001161 -0.00001392  1.        ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa2a2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[570.28690273, 174.51486652,   1.        ],\n",
       "       [624.65008891, 173.78645991,   1.        ],\n",
       "       [477.36310545, 214.69109366,   1.        ],\n",
       "       [718.31686353, 211.35433896,   1.        ],\n",
       "       [539.80449357, 221.77588628,   1.        ],\n",
       "       [656.34930348, 220.15126262,   1.        ],\n",
       "       [475.66496524, 256.16723767,   1.        ],\n",
       "       [721.13235294, 252.65056996,   1.        ],\n",
       "       [429.47587304, 317.45661308,   1.        ],\n",
       "       [768.656754  , 312.36021777,   1.        ],\n",
       "       [512.88752656, 461.33019207,   1.        ],\n",
       "       [689.6869646 , 458.37748774,   1.        ],\n",
       "       [510.77815285, 470.75196659,   1.        ],\n",
       "       [692.04307076, 467.70504463,   1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e99e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dt_apriltags as apriltag\n",
    "# apriltag.Detector(families='tagStandard52h13').detect(cv2.cvtColor(cv2.imread('marker.jpg'), cv2.COLOR_BGR2GRAY), estimate_tag_pose=True, camera_params=camera_params, tag_size=.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nm = cv2.imread(no_marker_name)\n",
    "\n",
    "for kp in transformed_anchors_points:\n",
    "    cv2.circle(img_nm, (int(kp[0]), int(kp[1])), 2, (0,0,255), -1)\n",
    "    \n",
    "# for kp in anchor_ground_truth:\n",
    "#     cv2.circle(img_nm, (int(kp[0]), int(kp[1])), 2, (255,0,0), -1)\n",
    "cv2.imshow('img with keypoints', img_nm)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2923570",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.Canny??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/marker_close_new.jpg')\n",
    "res = utils.detect_apriltag(img, camera_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8addc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in res[0].corners:\n",
    "    cv2.circle(img, (int(c[0]), int(c[1])), 2, (0,0, 255))\n",
    "    \n",
    "cv2.imshow('corners',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_without_marker_name = 'no_marker_close_new_2.jpg'\n",
    "img = cv2.imread(img_without_marker_name)\n",
    "\n",
    "# edges = cv2.Canny(img,40,100, True)\n",
    "# cv2.imshow('edges', edges)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# kernel = np.ones((5,5),np.float32)/25\n",
    "# dst = cv2.filter2D(edges,-1,kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circles_by_order(circles):\n",
    "    horizontality_thres = 7\n",
    "    new_order = np.zeros_like(circles)\n",
    "    mean = np.mean(circles, axis=0)\n",
    "    \n",
    "    def dist(a, b):\n",
    "        return abs(a[0]-b[0]) + abs(a[1]-b[1])\n",
    "    \n",
    "    dists = [dist(circle, mean) for circle in circles]\n",
    "    center = np.argmin(dists)\n",
    "    new_order[0] = circles[center]\n",
    "    \n",
    "    for i, circle in enumerate(circles):\n",
    "        if i == center: continue\n",
    "        if circle[0] < new_order[0][0]:\n",
    "            if abs(circle[1] - new_order[0][1]) < horizontality_thres:\n",
    "                new_order[3] = circle\n",
    "            elif circle[1] < new_order[0][1]:\n",
    "                new_order[1] = circle\n",
    "            else:\n",
    "                new_order[5] = circle\n",
    "        else:\n",
    "            if abs(circle[1] - new_order[0][1]) < horizontality_thres:\n",
    "                new_order[4] = circle\n",
    "            elif circle[1] < new_order[0][1]:\n",
    "                new_order[2] = circle\n",
    "            else:\n",
    "                new_order[6] = circle\n",
    "    return new_order\n",
    "\n",
    "circles = cv2.HoughCircles(dst, cv2.HOUGH_GRADIENT, dp=1.5, maxRadius=45, minRadius=15, \n",
    "                           minDist=30, param2=80)\n",
    "out = img.copy()\n",
    "print(circles)\n",
    "for x, y, r in circles[0]:\n",
    "    cv2.circle(out, (int(x), int(y)), int(r), (0, 255, 0), 4)\n",
    "    cv2.rectangle(out, (int(x) - 2, int(y) - 2), (int(x) + 2, int(y) + 2), (0, 128, 255), -1)\n",
    "cv2.imshow('circles', out)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "circles = np.array([(int(c[0]), int(c[1])) for c in circles[0]])\n",
    "anchor_ground_truth = circles_by_order(circles)\n",
    "anchor_ground_truth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
